{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Managing and evaluating generative AI systems\n",
    "\n",
    "Date: 25 Sept 2025\n",
    "\n",
    "## GenAIOps vs MLOps\n",
    "\n",
    "### Traditional MLOps Learning Lifecycle\n",
    "\n",
    "1. Business problem\n",
    "2. Goal definition: goal to solve the problem but also goal in terms of latency, etc. So you can monitor if it's not detrimental for the UX for example. You define the baseline of the model: the model will be good at these numbers.\n",
    "3. Data collection and prep: takes 80% of the work! very important to test the data and understand how many data points you need to build a good model (you need many data, otherwise GIGO!) You need to have a proper data lake with data monitoring in place before building the model, because if you have bad data, you won't have a good model. Also beware of GDPR when choosing the data, no sensitive data into the model!\n",
    "4. Feature engineering\n",
    "5. Model training\n",
    "6. Model evaluation: if the expected range is not met (i.e. the model is overfitting or underfitting), we go back to step (3) and loop until we pass evaluation\n",
    "7. Model registration: where you document the model, so that it is reproducible!\n",
    "8. Model deployment and serving\n",
    "9. Model monitoring: monitor that there is no data drift, that the model is still working at the right range that we set in our goal definition (baseline). Very crucial to ensure that the new trends are capture and see if new data make the model drift or continue to product the expected results within our acceptable margin (based on our goals).\n",
    "10. Model improvement --> back to step (3) and loop: this means that a model needs to be scalable because a model needs to be retrained continuously, so you need to feed in new data continuously --> it has to be scalable!\n",
    "\n",
    "### LLMLOps Lifecycle\n",
    "\n",
    "1. Business problem\n",
    "2. Goal definition\n",
    "3. Data collection \n",
    "4. Feature engineering done by the big players (Mistral, OpenAI, Anthropic, etc.): We are not building models anymore, we use existing models\n",
    "5. Optional: Fine-tuning of existing models (open source only)\n",
    "5. Deployment of models (as is or fine-tuned)\n",
    "6. Use the model directly with prompt engineering, or use agents, or RAG\n",
    "7. Evaluation: very important because we need to be sure that things happen how we defined them in our goals\n",
    "\n",
    "### Foundational models\n",
    "\n",
    "The future is larger models (more and more billions of parameters; e.g. from GPT-2 to GPT-3 we went from 1B of parameters to 100B of parameters) but also smaller models that are more specialized for specific industries and produce better results because their data for training is more focused. They will use smaller models from big players and fine tune them; not build new foundational models from scratch because too costly (GPUs, etc.; cost of training GPT-3.5: 20M USD), very heavy work and you don't even know if it's going to work at the end.\n",
    "\n",
    "So alternatives are:\n",
    "- Fine-Tuning\n",
    "- Prompt Engineering\n",
    "- RAG\n",
    "\n",
    "### Fine-Tuning\n",
    "\n",
    "2 ways to do it:\n",
    "1. Parameter-Efficient Fine-Tuning (PEFT): Freeze the pretrained transformer (with specific weight, etc.) and add new layers on top (easiest); update only parts of the model\n",
    "2. Full Fine-Tuning: Retrain the pretrained transformer by adding new layers and revisit then all parameters\n",
    "\n",
    "Fine-tuning continues the training of the model on a smaller, narrower dataset. \n",
    "\n",
    "**Fine-tuning vs. RAG:**\n",
    "With fine-tuning the context is built into the model vs. RAG: add context on top of the model.\n",
    "With fine-tuning once the data is in there, you can't change it vs. RAG: you can change data anytime because you can always choose which context you pass.\n",
    "\n",
    "To really fine-tune a big model we would need GPU, etc. We can do fine-tuning on our own computer but only with small models. E.g.:\n",
    "- Fine-tuning LLaMA 7B parameters with 1GPU and 25M total tokens (size of dataset x nbr of times we run it (epochs)): it will take over 5h to fine-tune and cost 15USD\n",
    "- Fine-tuning GPT3 175B parameters with 32GPU (to be faster) and 500M tokens: it will take 2600h to fine-tune and cost 200k USD!\n",
    "\n",
    "Fine-tuning is good to use when the data we want to add are not going to evolve, e.g. legal data, medical, etc. \n",
    "\n",
    "Always start small and specific, then evaluate, then move up. Never start with a big model!\n",
    "\n",
    "!!! In real case scenario, you can do fine-tuning directly in Amazon Bedrock.\n",
    "\n",
    "### Deployment strategies\n",
    "\n",
    "Deployment can be:\n",
    "- serverless (fully managed)\n",
    "- dedicated instance on cloud (e.g. dedicated instance of OpenAI models on Azure cloud -> so the data you send to these models is not going to OpenAI, only to your instance of chatGPT)\n",
    "- self-hosted\n",
    "\n",
    "Best approach for cost and easy: serverless\n",
    "Best approach for privacy sensitive: dedicated instance\n",
    "\n",
    "**Scaling**\n",
    "- Caching: Improve scaling with caching (e.g. Redis, Memcached, vector DBs).\n",
    "- Batch inferencing: Scaling can be improved as well with batch processing --> this is good when latency is not key, don't have to do it live.\n",
    "- Even with Live inference we can have batch techniques:\n",
    "    -- fixed-size batching (process nbr of requests together with max, e.g. N = 32)\n",
    "    -- time-window batching (wait for a fixed time interval and process together, e.g. T = 50ms)\n",
    "    -- dynamic batching (group requests together wihin a max tokens number, e.g. 2048 tokens)\n",
    "\n",
    "### LLM Serving\n",
    "\n",
    "You need to have a Gateway / proxy layer between your user and the models.\n",
    "\n",
    "Gateway with:\n",
    "- authentication\n",
    "- rate limiting\n",
    "- guardrails (including exclusion of sensitive data, including content moderation, have clear guardrails against nudity, self harm, etc.)\n",
    "- cost\n",
    "- monitoring\n",
    "\n",
    "This Gateway can route the traffic to the proper model.\n",
    "\n",
    "Read this article: GenAI proxy at Expedia: https://medium.com/expedia-group-tech/gateways-guardrails-and-genai-models-aa606379164d\n",
    "Now we have LiteLLM (https://www.litellm.ai/#features) that provides this kind of service or https://www.guardrailsai.com/\n",
    "\n",
    "## Prompt Engineering and Management\n",
    "\n",
    "70% of the problems can be solved by prompt engineering!\n",
    "\n",
    "Also GIGO with prompts!\n",
    "\n",
    "**Chain-of-thought prompting:** e.g. doctor diagnosis --> similar to fundraising diagnosis? give the info (anamnÃ¨se) then ask the LLM to do the diagnosis step by step.\n",
    "\n",
    "Do three main instructions, then fine tune the prompt based on the output we get, section by section. We do it in an iterative way instead of putting too much info that will clutter the context window!\n",
    "\n",
    "**Template prompting:** e.g. sentiment analysis\n",
    "Check the prompt builders in chatGPT (custom GPTs)\n",
    "\n",
    "Always start with the simplest:\n",
    "1. Prompt engineering --> if does not give proper output, move to (2)\n",
    "2. Fine-tuning --> if does not give proper output and if you have the resources!, move to (3)\n",
    "3. Pre-training (creating your own model)\n",
    "\n",
    "#### Prompt Management\n",
    "\n",
    "1. Prompt repository\n",
    "2. Prompt version control\n",
    "3. Prompt evaluation\n",
    "4. Prompt collaboration\n",
    "5. LLM integration\n",
    "\n",
    "We treat the prompts as code!\n",
    "\n",
    "Tools for version control: *Langfuse* (https://langfuse.com/), *Braintrust* (https://www.braintrust.dev/), *Langsmith*, *PromptLayer*, GitHub repo.\n",
    "Braintrust includes evals as well right in it.\n",
    "\n",
    "Prompt templates, memory and chains: LangChain.\n",
    "\n",
    "### Ollama\n",
    "\n",
    "Enables you to download distillations of open source models to run them directly on your machine.\n",
    "Only small models, so you wouldn't use Ollama for production (because it's local AND it's small models, max. 8B parameters to run on our machine; very good gamine computer can run 32B parameters), but good to run tests.\n",
    "\n",
    "---\n",
    "\n",
    "## RAG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
