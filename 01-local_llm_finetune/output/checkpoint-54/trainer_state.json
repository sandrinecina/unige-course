{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 18.0,
  "eval_steps": 500,
  "global_step": 54,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 10.472312927246094,
      "learning_rate": 0.0004,
      "loss": 7.9829,
      "step": 1
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 18.498950958251953,
      "learning_rate": 0.0003925925925925926,
      "loss": 11.1804,
      "step": 2
    },
    {
      "epoch": 1.0,
      "grad_norm": 21.908613204956055,
      "learning_rate": 0.0003851851851851852,
      "loss": 5.195,
      "step": 3
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 16.149072647094727,
      "learning_rate": 0.00037777777777777777,
      "loss": 4.3898,
      "step": 4
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 24.90778923034668,
      "learning_rate": 0.0003703703703703704,
      "loss": 4.3262,
      "step": 5
    },
    {
      "epoch": 2.0,
      "grad_norm": 23.362295150756836,
      "learning_rate": 0.000362962962962963,
      "loss": 4.8887,
      "step": 6
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 8.078910827636719,
      "learning_rate": 0.00035555555555555557,
      "loss": 3.5932,
      "step": 7
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 21.896514892578125,
      "learning_rate": 0.00034814814814814816,
      "loss": 3.4766,
      "step": 8
    },
    {
      "epoch": 3.0,
      "grad_norm": 7.555960655212402,
      "learning_rate": 0.00034074074074074074,
      "loss": 2.2873,
      "step": 9
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 8.743767738342285,
      "learning_rate": 0.0003333333333333334,
      "loss": 2.1813,
      "step": 10
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 4.886246681213379,
      "learning_rate": 0.0003259259259259259,
      "loss": 2.9988,
      "step": 11
    },
    {
      "epoch": 4.0,
      "grad_norm": 3.061751127243042,
      "learning_rate": 0.00031851851851851854,
      "loss": 1.9201,
      "step": 12
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 2.790222406387329,
      "learning_rate": 0.0003111111111111111,
      "loss": 1.8326,
      "step": 13
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 2.7558093070983887,
      "learning_rate": 0.0003037037037037037,
      "loss": 1.6678,
      "step": 14
    },
    {
      "epoch": 5.0,
      "grad_norm": 4.104074954986572,
      "learning_rate": 0.0002962962962962963,
      "loss": 2.7582,
      "step": 15
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 4.232227802276611,
      "learning_rate": 0.0002888888888888889,
      "loss": 2.6844,
      "step": 16
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 2.631499767303467,
      "learning_rate": 0.0002814814814814815,
      "loss": 1.5208,
      "step": 17
    },
    {
      "epoch": 6.0,
      "grad_norm": 2.3440327644348145,
      "learning_rate": 0.0002740740740740741,
      "loss": 1.4728,
      "step": 18
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 2.3238515853881836,
      "learning_rate": 0.0002666666666666667,
      "loss": 1.4235,
      "step": 19
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 2.5234289169311523,
      "learning_rate": 0.00025925925925925926,
      "loss": 1.3035,
      "step": 20
    },
    {
      "epoch": 7.0,
      "grad_norm": 2.9519026279449463,
      "learning_rate": 0.00025185185185185185,
      "loss": 2.3161,
      "step": 21
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 2.3115639686584473,
      "learning_rate": 0.0002444444444444445,
      "loss": 1.2423,
      "step": 22
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 2.4330825805664062,
      "learning_rate": 0.00023703703703703704,
      "loss": 1.0975,
      "step": 23
    },
    {
      "epoch": 8.0,
      "grad_norm": 3.2402215003967285,
      "learning_rate": 0.00022962962962962965,
      "loss": 2.1351,
      "step": 24
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 3.3159701824188232,
      "learning_rate": 0.00022222222222222223,
      "loss": 2.0678,
      "step": 25
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 2.4591774940490723,
      "learning_rate": 0.00021481481481481484,
      "loss": 0.9197,
      "step": 26
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.770895481109619,
      "learning_rate": 0.0002074074074074074,
      "loss": 0.9951,
      "step": 27
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 3.4526619911193848,
      "learning_rate": 0.0002,
      "loss": 1.8519,
      "step": 28
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 2.898050546646118,
      "learning_rate": 0.0001925925925925926,
      "loss": 0.9015,
      "step": 29
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.3853583335876465,
      "learning_rate": 0.0001851851851851852,
      "loss": 0.7197,
      "step": 30
    },
    {
      "epoch": 10.333333333333334,
      "grad_norm": 2.3622541427612305,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.6712,
      "step": 31
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 3.477262020111084,
      "learning_rate": 0.00017037037037037037,
      "loss": 1.6394,
      "step": 32
    },
    {
      "epoch": 11.0,
      "grad_norm": 3.183384656906128,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.7348,
      "step": 33
    },
    {
      "epoch": 11.333333333333334,
      "grad_norm": 3.135608673095703,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.6944,
      "step": 34
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 2.575000524520874,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.5041,
      "step": 35
    },
    {
      "epoch": 12.0,
      "grad_norm": 4.135245323181152,
      "learning_rate": 0.00014074074074074076,
      "loss": 1.4801,
      "step": 36
    },
    {
      "epoch": 12.333333333333334,
      "grad_norm": 4.15418004989624,
      "learning_rate": 0.00013333333333333334,
      "loss": 1.4439,
      "step": 37
    },
    {
      "epoch": 12.666666666666666,
      "grad_norm": 3.1603925228118896,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.5415,
      "step": 38
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.856355905532837,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.3732,
      "step": 39
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 4.493416786193848,
      "learning_rate": 0.00011111111111111112,
      "loss": 1.3104,
      "step": 40
    },
    {
      "epoch": 13.666666666666666,
      "grad_norm": 3.031511068344116,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.3173,
      "step": 41
    },
    {
      "epoch": 14.0,
      "grad_norm": 3.6280057430267334,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.4258,
      "step": 42
    },
    {
      "epoch": 14.333333333333334,
      "grad_norm": 3.141799211502075,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.2453,
      "step": 43
    },
    {
      "epoch": 14.666666666666666,
      "grad_norm": 5.620699882507324,
      "learning_rate": 8.148148148148148e-05,
      "loss": 1.1487,
      "step": 44
    },
    {
      "epoch": 15.0,
      "grad_norm": 4.001245975494385,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.3586,
      "step": 45
    },
    {
      "epoch": 15.333333333333334,
      "grad_norm": 6.5498552322387695,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.0743,
      "step": 46
    },
    {
      "epoch": 15.666666666666666,
      "grad_norm": 3.2054431438446045,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.1552,
      "step": 47
    },
    {
      "epoch": 16.0,
      "grad_norm": 5.498703956604004,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.3101,
      "step": 48
    },
    {
      "epoch": 16.333333333333332,
      "grad_norm": 3.0429818630218506,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.1312,
      "step": 49
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 7.778146743774414,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.9669,
      "step": 50
    },
    {
      "epoch": 17.0,
      "grad_norm": 4.8528289794921875,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.27,
      "step": 51
    },
    {
      "epoch": 17.333333333333332,
      "grad_norm": 4.408504486083984,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.262,
      "step": 52
    },
    {
      "epoch": 17.666666666666668,
      "grad_norm": 8.22083568572998,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.9216,
      "step": 53
    },
    {
      "epoch": 18.0,
      "grad_norm": 2.3585922718048096,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.0986,
      "step": 54
    }
  ],
  "logging_steps": 1,
  "max_steps": 54,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 18,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 21475005825024.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
